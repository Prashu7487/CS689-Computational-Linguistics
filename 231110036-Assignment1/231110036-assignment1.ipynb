{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266bba62",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    " > Please Specify correct path for each file and output files will be in same directory\n",
    " \n",
    " > 3 files are expected to run this notebook all at once ('hi_100.txt' ,'cs689_assignment.txt' , 'ground_truth_tokens.txt')\n",
    " \n",
    " > Restart and Run all the Kernal to execute whole notebook at once\n",
    " \n",
    " > The function named 'write_to_file_and_print_top_k' writes output of some models also\n",
    "     print top-k values (based on freq) in current directory, Please comment that if no file is expected\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9f556",
   "metadata": {},
   "source": [
    "# Problem-1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "509e2f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['आवेदन करने की आखिरी तारीख 31 जनवरी, 2020 है।\\n', 'इतनी दुआ कर दो हमारे लिए कि जितना प्यार दुनिया ने आपको दिया है, बस उतना ही हमें भी मिल जाए|”\\n', 'मोदी सरकार के पहले कार्यकाल में भी तीन तलाक को लेकर बिल लाया गया था, हालांकि तब यह राज्यसभा में पास नहीं हो पाया था.\\n', 'भाजपा के दिवंगत नेता प्रमोद महाजन की बेटी पूनम महाजन को सचिव बनाया गया है.\\n', \"ऐसी स्थिति में एक न्यायपूर्ण सरकार सार्वजनिक वित्त का इस तरह इस्तेमाल करती है कि संसाधनों का आवंटन, सभी के उपभोग वाले उत्पादों की व्यवहार्यता और समग्र वृहद-आर्थिक प्रबंधन 'निष्पक्षता के रूप में न्याय' को बढ़ाए।\\n\"]\n"
     ]
    }
   ],
   "source": [
    "file_path = 'hi_100.txt'  \n",
    "try:\n",
    "    with open(file_path, 'r',encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "        print(data[:5])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c53cb00",
   "metadata": {},
   "source": [
    "### function for Unicode Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148312a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets = {\n",
    "    \"क\": ['क्', 'अ'], \"ख\": ['ख्', 'अ'], \"ग\": ['ग्', 'अ'], \"घ\": ['घ्', 'अ'], \"ङ\": ['ङ्', 'अ'],\n",
    "    \"च\": ['च्', 'अ'], \"छ\": ['छ्', 'अ'], \"ज\": ['ज्', 'अ'], \"झ\": ['झ्', 'अ'], \"ञ\": ['ञ्', 'अ'],\n",
    "    \"ट\": ['ट्', 'अ'], \"ठ\": ['ठ्', 'अ'], \"ड\": ['ड्', 'अ'],\"ड़\":['ड़'] ,\"ढ\": ['ढ्', 'अ'], \"ण\": ['ण्', 'अ'],\n",
    "    \"त\": ['त्', 'अ'], \"थ\": ['थ्', 'अ'], \"द\": ['द्', 'अ'], \"ध\": ['ध्', 'अ'], \"न\": ['न्', 'अ'],\n",
    "    \"प\": ['प्', 'अ'], \"फ\": ['फ्', 'अ'], \"ब\": ['ब्', 'अ'], \"भ\": ['भ्', 'अ'], \"म\": ['म्', 'अ'],\n",
    "    \"य\": ['य्', 'अ'], \"र\": ['र्', 'अ'], \"ल\": ['ल्', 'अ'], \"व\": ['व्', 'अ'],\n",
    "    \"श\": ['श्', 'अ'], \"ष\": ['ष्', 'अ'], \"स\": ['स्', 'अ'], \"ह\": ['ह्', 'अ'],\n",
    "    \"क्ष\": ['क्', 'ष्', 'अ'], \"त्र\": ['त्', 'र्', 'अ'],\n",
    "    \"ज्ञ\": ['ज्', 'ञ्', 'अ'],\n",
    "    \"अ\": ['अ'], \"आ\": ['आ'], \"इ\": ['इ'], \"ई\": ['ई'], \"उ\": ['उ'], \"ऊ\": ['ऊ'],\n",
    "    \"ऋ\": ['ऋ'], \"ॠ\": ['ॠ'], \"ऌ\": ['ऌ'], \"ॡ\": ['ॡ'],\n",
    "    \"ए\": ['ए'], \"ऐ\": ['ऐ'], \"ओ\": ['ओ'], \"औ\": ['औ'],\n",
    "    \"ा\": ['अ'], \"ि\": ['इ'], \"ी\": ['ई'], \"ु\": ['उ'], \"ू\": ['ऊ'],\n",
    "    \"ृ\": ['ऋ'], \"ॄ\": ['ॠ'], \"ॢ\": ['ऌ'], \"ॣ\": ['ॡ'],\n",
    "    \"े\": ['ए'], \"ै\": ['ऐ'],\n",
    "    \"ो\": ['ओ'], \"ौ\": ['औ'],\n",
    "    \"ं\": ['अं'], \"ः\": ['अः'], \"ँ\": ['अँ'], '१':['१'], '२':['२'], '३':['३'], '४':['४'], '५':['५'], '६':['६'], '७':['७'],\n",
    "    '८':['८'], '९':['९']\n",
    "}\n",
    "\n",
    "reverse_matras = {\"अ\": \"\", \"आ\": \"ा\", \"इ\": \"ि\", \"ई\": \"ी\", \"उ\": \"ु\", \"ऊ\": \"ू\", \"ऋ\": \"ृ\", \"ॠ\": \"ॄ\", \"ऌ\": \"ॢ\", \"ॡ\": \"ॣ\",\n",
    "                  \"ए\": \"े\", \"ऐ\": \"ै\", \"ओ\": \"ो\", \"औ\": \"ौ\", \"अं\": \"ं\", \"अः\": \"ः\", \"अँ\": \"ँ\"}\n",
    "second_matras = {\"अं\": \"ं\", \"अः\": \"ः\", \"अँ\": \"ँ\"}\n",
    "reverse_matras_second = {\"ं\": \"अं\", \"ः\": \"अः\", \"ँ\": \"अँ\"}\n",
    "consonant_mapping = {\n",
    "    \"क\": \"k\", \"ख\": \"kh\", \"ग\": \"g\", \"घ\": \"gh\", \"ङ\": \"N\",\n",
    "    \"च\": \"ch\", \"छ\": \"chh\", \"ज\": \"j\", \"झ\": \"jh\", \"ञ\": \"~N\",\n",
    "    \"ट\": \"T\", \"ठ\": \"Th\", \"ड\": \"D\", \"ढ\": \"Dh\", \"ण\": \"N\",\n",
    "    \"त\": \"t\", \"थ\": \"th\", \"द\": \"d\", \"ध\": \"dh\", \"न\": \"n\",\n",
    "    \"प\": \"p\", \"फ\": \"ph\", \"ब\": \"b\", \"भ\": \"bh\", \"म\": \"m\",\n",
    "    \"य\": \"y\", \"र\": \"r\", \"ल\": \"l\", \"व\": \"v\", \"श\": \"sh\",\n",
    "    \"ष\": \"Sh\", \"स\": \"s\", \"ह\": \"h\", \"ळ\": \"L\", \"क्ष\": \"kSh\",\"ड़\":\"D\",\n",
    "    \"ज्ञ\": \"jyn\", \"ड़\": \"R\", \"य़\": \"y\", \"ज़\": \"z\", \"ब़\": \"b\", \"क़\": \"q\", \"ख़\": \"Kh\", \"ग़\": \"G\",\n",
    "    \"ड़\": \"R\", \"ढ़\": \"Rh\", \"फ़\": \"f\", \"श़\": \"sh\", \"ऋ\": \"Ri\", \"ॠ\": \"Ri\", \"ॡ\": \"Li\", \"ऌ\": \"Li\", \"ऴ\": \"L\", \"ॐ\": \"OM\",\n",
    "    \"ऽ\": \"'\"\n",
    "}\n",
    "\n",
    "matras = {\"ा\": \"आ\", \"ि\": \"इ\", \"ी\": \"ई\", \"ु\": \"उ\", \"ू\": \"ऊ\", \"ृ\": \"ऋ\", \"ॄ\": \"ॠ\", \"ॢ\": \"ऌ\", \"ॣ\": \"ॡ\", \"े\": \"ए\", \"ै\": \"ऐ\",\n",
    "          \"ो\": \"ओ\", \"ौ\": \"औ\", \"ं\": \"अं\", \"ः\": \"अः\", \"ँ\": \"अँ\"}\n",
    "\n",
    "halanta_char =  {\n",
    "    \"क्\": 'क', \"ख्\": 'ख', \"ग्\": 'ग', \"घ्\": 'घ', \"ङ्\": 'ङ',\n",
    "    \"च्\": 'च', \"छ्\": 'छ', \"ज्\": 'ज', \"झ्\": 'झ', \"ञ्\": 'ञ',\n",
    "    \"ट्\": 'ट', \"ठ्\": 'ठ', \"ड्\": 'ड', \"ढ्\": 'ढ', \"ण्\": 'ण',\n",
    "    \"त्\": 'त', \"थ्\": 'थ', \"द्\": 'द', \"ध्\": 'ध', \"न्\": 'न',\n",
    "    \"प्\": 'प', \"फ्\": 'फ', \"ब्\": 'ब', \"भ्\": 'भ', \"म्\": 'म',\n",
    "    \"य्\": 'य', \"र्\": 'र', \"ल्\": 'ल', \"व्\": 'व',\n",
    "    \"श्\": 'श', \"ष्\": 'ष', \"स्\": 'स', \"ह्\": 'ह',\n",
    "    \"ळ्\": 'ळ', \"क्ष्\": 'क्ष', \"ज्ञ्\": 'ज्ञ', \"ड़्\": 'ड़', \"य़्\": 'य़',\n",
    "    \"ज़्\": 'ज़', \"ब़्\": 'ब़', \"क़्\": 'क़', \"ख़्\": 'ख़', \"ग़्\": 'ग़',\n",
    "    \"ड़्\": 'ड़', \"ढ़्\": 'ढ़', \"फ़्\": 'फ़', \"श़्\": 'श़',\n",
    "    \"ऋ्\": 'ऋ', \"ॠ्\": 'ॠ', \"ॡ्\": 'ॡ', \"ऌ्\": 'ऌ', \"ऴ्\": 'ऴ',\n",
    "    \"ॐ्\": 'ॐ', \"ऽ्\": 'ऽ'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e2e1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_conversion(line):\n",
    "    chars_list = []\n",
    "    i = 0\n",
    "        \n",
    "    drop_list = symbols_list = ['।', ' ', ',', '०', '.', '?', '!', '-', ':', ';']\n",
    "    while i < len(line):\n",
    "        current_char = line[i]\n",
    "        if(current_char in drop_list):\n",
    "            i+=1\n",
    "            continue\n",
    "        next_char = line[i + 1] if i + 1 < len(line) else None\n",
    "        next_next_char = line[i + 2] if i + 2 < len(line) else None\n",
    "\n",
    "        if current_char in consonant_mapping and next_char not in matras:\n",
    "            if current_char in alphabets:\n",
    "                chars_list.extend(alphabets[current_char])\n",
    "            else:\n",
    "                chars_list.extend(current_char)\n",
    "        elif current_char in matras:\n",
    "            chars_list.append(matras[current_char])\n",
    "        else:\n",
    "            chars_list.extend(current_char)\n",
    "        i += 1\n",
    "\n",
    "    chars_list = [char for char in chars_list if char != \" \"]  # Remove empty characters\n",
    "    return chars_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0eaffcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['स', 'ऊ', 'र्', 'अ', 'ज्', 'अ', 'क', 'ई', 'क', 'इ', 'र्', 'अ', 'ण', 'ओ', 'अं', 'न', 'ए', 'स्', 'अ', 'म', 'उ', 'द्', 'अ', '्', 'र्', 'अ', 'क', 'ई', 'ल्', 'अ', 'ह्', 'अ', 'र', 'ओ', 'अं', 'क', 'ओ', 'च्', 'अ', 'म्', 'अ', 'क', 'आ', 'य', 'आ'] count of characters is:  45\n"
     ]
    }
   ],
   "source": [
    "val = character_conversion(\"सूरज की किरणों ने समुद्र की लहरों को चमकाया\")\n",
    "print(val, \"count of characters is: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d572d5",
   "metadata": {},
   "source": [
    "# Problem-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd898067",
   "metadata": {},
   "source": [
    "### Function definations for syllable conversion and collecting top-20 highest freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8fc0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_freq_unigram(u_freq):\n",
    "    sorted_unigram_freq = sorted(u_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_20_unigram = dict(sorted_unigram_freq[:20])\n",
    "    return top_20_unigram\n",
    "\n",
    "def top_freq_bigram(b_freq):\n",
    "    sorted_bigram_freq = sorted(b_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_20_bigram = dict(sorted_bigram_freq[:20])\n",
    "    return top_20_bigram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83705c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['बा', 'रि', 'श', 'के', 'मौ', 'स', 'म', 'में', 'लो', 'ग', 'ग', 'र', 'म', 'चा', 'य', 'का', 'आ', 'नं', 'द', 'ले', 'ते', 'हैं'] count of syllable is:  22\n"
     ]
    }
   ],
   "source": [
    "#  This function expects args are chars so dependent on character_conversion inherently\n",
    "def syllable_convert(val):\n",
    "    val = character_conversion(val)\n",
    "    syllables = []\n",
    "    current_syllable = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while i < len(val):\n",
    "        if val[i] in consonant_mapping:\n",
    "            current_syllable += val[i]\n",
    "        elif val[i] in halanta_char:\n",
    "            current_syllable += halanta_char[val[i]]\n",
    "        else:\n",
    "            if current_syllable != \"\":\n",
    "                if val[i] in reverse_matras:\n",
    "                    current_syllable += reverse_matras[val[i]]\n",
    "                if i + 1 < len(val) and val[i + 1] in second_matras:\n",
    "                    current_syllable += second_matras[val[i + 1]]\n",
    "                    i += 1\n",
    "            else:\n",
    "                current_syllable = val[i]\n",
    "            syllables.append(current_syllable)\n",
    "            current_syllable = \"\"\n",
    "        i += 1\n",
    "\n",
    "    # Remove empty strings from the list\n",
    "    syllables = [x for x in syllables if x]\n",
    "\n",
    "    return syllables\n",
    "\n",
    "syll = syllable_convert(\"बारिश के मौसम में, लोग गरम चाय का आनंद लेते हैं।\")\n",
    "print(syll, \"count of syllable is: \", len(syll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e141cbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Syllable Unigram Frequency:\n",
      "\n",
      "{'र': 1403416, '्': 1385294, 'क': 775832, 'स': 686588, 'न': 599409, 'प': 546428, 'त': 473505, 'के': 399015, 'ल': 390598, 'म': 355766, 'ने': 325171, 'ए': 302617, 'का': 300554, '\\n': 298799, 'या': 284731, 'ह': 282565, 'य': 277701, 'ब': 273580, 'ग': 269704, 'में': 259827}\n",
      "\n",
      "Syllable Bigram Frequency:\n",
      "\n",
      "{'र ्': 230510, 'क र': 176396, 'स ्': 168036, '् र': 167055, 'क ्': 157814, 'प ्': 141222, 'त ्': 123838, 'औ र': 115698, 'प र': 107416, '् य': 91062, 'इ स': 88005, 'न ्': 83708, 'है \\n': 79522, '् या': 69781, 'ड ़': 65370, 'ए क': 65133, 'का र': 63436, '् ट': 59278, 'द ्': 58980, 'ल ्': 57341}\n",
      "\n",
      "Syllable Unigram Frequency (Character-based):\n",
      "\n",
      "{'अ': 8522539, 'आ': 2991109, 'ए': 2318442, 'ई': 1460305, 'क': 1442541, 'इ': 1432973, 'र्': 1403772, '्': 1385508, 'अं': 1201207, 'ओ': 896588, 'ह': 848785, 'क्': 777423, 'न': 734863, 'र': 711877, 'म': 697402, 'स्': 686835, 'न्': 599585, 'स': 596873, 'उ': 587149, 'प्': 547147}\n",
      "\n",
      "Syllable Bigram Frequency (Character-based):\n",
      "\n",
      "{'र् अ': 1403772, 'अ ्': 1382369, 'क् अ': 777423, 'स् अ': 686835, 'अ र्': 641278, 'अ क': 627243, 'न् अ': 599585, 'प् अ': 547147, 'त् अ': 480418, 'क ए': 407129, 'ल् अ': 390677, 'अ ह': 382660, 'ए अं': 359393, 'म् अ': 355835, 'अ न': 355715, 'न ए': 328953, 'अ म': 324744, 'अ क्': 319088, 'क आ': 314321, 'य आ': 297771}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "processed_data = [(character_conversion(line), syllable_convert(line)) for line in data]\n",
    "\n",
    "unigram_freq_char = Counter(char for val, _ in processed_data for char in val)\n",
    "unigram_freq = Counter(syllable for _, freq in processed_data for syllable in freq)\n",
    "\n",
    "bigram_freq_char = Counter(\" \".join((val[i], val[i + 1])) for val, _ in processed_data for i in range(len(val) - 1))\n",
    "bigram_freq = Counter(\" \".join((freq[i], freq[i + 1])) for _, freq in processed_data for i in range(len(freq) - 1))\n",
    "\n",
    "# Print top 20 values\n",
    "print(\"\\nSyllable Unigram Frequency:\\n\")\n",
    "print(dict(unigram_freq.most_common(20)))\n",
    "\n",
    "print(\"\\nSyllable Bigram Frequency:\\n\")\n",
    "print(dict(bigram_freq.most_common(20)))\n",
    "\n",
    "print(\"\\nCharacter Unigram Frequency :\\n\")\n",
    "print(dict(unigram_freq_char.most_common(20)))\n",
    "\n",
    "print(\"\\nCharacter Bigram Frequency :\\n\")\n",
    "print(dict(bigram_freq_char.most_common(20)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be203f4d",
   "metadata": {},
   "source": [
    "# Problem-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "558fa778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempted manually on site \"https://bangla.iitk.ac.in/cs689/main\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764504b",
   "metadata": {},
   "source": [
    "# Problem-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5390084c",
   "metadata": {},
   "source": [
    "### Reading Data File 'hi_100.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4698cc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "आवेदन करने की आखिरी तारीख 31 जनवरी, 2020 है।\n",
      "इतनी दुआ कर दो हमारे लिए कि जितना प्यार दुनिया ने आपको दिया है, बस उतना ही हमें भी मिल जाए|”\n",
      "मोदी सरकार के पहले कार्यकाल में भी तीन तलाक को लेकर बिल लाया गया था, हालांकि तब यह राज्यसभा में पास नहीं हो पाया था.\n",
      "भाजपा के दिवंगत नेता प्रमोद महाजन की बेटी पूनम महाजन को सचिव बनाया गया है.\n",
      "ऐसी स्थिति में एक न्यायपूर्ण सरकार सार्वजनिक वित्त का इस तरह इस्तेमाल करती है कि संसाधनों का आवंटन, सभी के उपभोग वाले उत्पादों की व्यवहार्यता और समग्र वृहद-आर्थिक प्रबंधन 'निष्पक्षता के रूप में न्याय' को बढ़ाए।\n",
      "दिलचस्प है कि डीसीएचएल के चेयरमैन टी वेंकटरमन रेड्डी और वाइस चेयरमैन टी विनायक रवि रेड्डी इस बैठक में मौजूद नहीं थे।\n",
      "इस आम चुनाव में भाजपा नेता सतीश कुमार गौतम को सबसे अधिक 6 लाख 56 हजार 215 वोट प्राप्त हुए.\n",
      "आयरलैंड टीम के विकेटकीपर बल्लेबाज नियाल ओ'ब्रायन ने अंतर्राष्ट्रीय क्रिकेट से संन्यास का ऐलान कर दिया है। साल 2002 में डेनमार्क के खिलाफ के एकदिवसीय क्रिकेट में डेब्यू करने वाले ब्रायन ने 36 साल की उम्र में संन्यास का ऐलान किया। नियाल ओ'ब्रायन आयरलैंड\n"
     ]
    }
   ],
   "source": [
    "file_path = 'hi_100.txt'  \n",
    "try:\n",
    "    with open(file_path, 'r',encoding='utf-8') as file:\n",
    "        data = file.read()\n",
    "        print(data[:1000])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ce14dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Uncomment this line for running model in less time, but models will not be trained on whole corpus\n",
    "# data = data[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111cca63",
   "metadata": {},
   "source": [
    "### Function definations for calculating top k freqs and writing them into files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce480c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file_and_print_top_k(model_name, vocab, k, freq_arr):\n",
    "    \"\"\"\n",
    "    Write token frequencies to files and print the top k tokens for each gram type.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name (str): The name of the model.\n",
    "    - vocab (str): The vocabulary name.\n",
    "    - k (int): The number of top tokens to display.\n",
    "    - freq_arr (tuple): A tuple containing four dictionaries of token frequencies\n",
    "                       for unigrams, bigrams, syllables, and characters.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for grams, gram_type in [(freq_arr[i], [\"unigram\", \"bigram\", \"syllables\", \"char\"][i]) for i in range(4)]:\n",
    "        sorted_grams = sorted(grams.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "#         Uncomment below line will write all results in the file with respective names\n",
    "        with open(f\"{model_name}_{gram_type}_{vocab}.txt\", 'w', encoding='utf-8') as f:\n",
    "            f.writelines([f\"{gram}: {count}\\n\" for gram, count in sorted_grams])\n",
    "\n",
    "        print(f\"\\nTop {k} For {gram_type}\\n\")\n",
    "        print(\"\\n\".join([f\"{token}: {freq}\" for token, freq in sorted_grams[:k]]))\n",
    "\n",
    "\n",
    "        \n",
    "def count_frequencies(tokenizer, data):\n",
    "    \"\"\"\n",
    "    Count the frequencies of unigrams, bigrams, syllables, and characters in the given data.\n",
    "\n",
    "    Parameters:\n",
    "    - tokenizer (Tokenizer): A tokenizer object to encode the data.\n",
    "    - data (str): The input data for frequency analysis.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing four Counter objects representing the frequencies of unigrams, bigrams,\n",
    "           syllable bigrams, and character bigrams in the given data.\n",
    "    \"\"\"\n",
    "    # generating tokens in data by tokenizer\n",
    "    tokens = tokenizer.encode(data, out_type=str)\n",
    "    \n",
    "    # Count unigram frequencies\n",
    "    unigram_freqs = Counter(tokens)\n",
    "    \n",
    "    # Count bigram frequencies\n",
    "    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "    bigram_freqs = Counter(bigrams)\n",
    "\n",
    "    # generating syllables and characters of tokens\n",
    "    syllables = []\n",
    "    characters = []\n",
    "    for tok in tokens:\n",
    "        syllables.extend(syllable_convert(tok))\n",
    "        characters.extend(character_conversion(tok))\n",
    "        \n",
    "    bigram_syllables = [(syllables[i], syllables[i + 1]) for i in range(len(syllables) - 1)]\n",
    "    bigram_characters = [(characters[i], characters[i + 1]) for i in range(len(characters) - 1)]\n",
    "    \n",
    "    syllable_freqs = Counter(bigram_syllables)\n",
    "    char_freqs = Counter(bigram_characters)\n",
    "\n",
    "    return unigram_freqs, bigram_freqs, syllable_freqs, char_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320031d8",
   "metadata": {},
   "source": [
    "### Function definations to return Unigram and BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2419e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from collections import Counter\n",
    "\n",
    "def train_unigram_tokenizer(file_path, vocab_size):\n",
    "    \"\"\"\n",
    "    Train and return a SentencePiece unigram tokenizer using the specified file_path and vocab_size.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the input file for training the tokenizer.\n",
    "    - vocab_size (int): The desired vocabulary size for the trained tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    SentencePieceProcessor: A trained unigram tokenizer.\n",
    "    \"\"\"\n",
    "    # Train unigram tokenizer\n",
    "    spm.SentencePieceTrainer.Train(input=file_path, model_prefix='unigram_model', model_type='unigram', vocab_size=vocab_size)\n",
    "\n",
    "    # Load the trained unigram tokenizer\n",
    "    model_file = 'unigram_model.model'\n",
    "    unigram_tokenizer = spm.SentencePieceProcessor(model_file)\n",
    "    return unigram_tokenizer\n",
    "\n",
    "def train_bpe_tokenizer(file_path, vocab_size):\n",
    "    \"\"\"\n",
    "    Train and return a SentencePiece BPE tokenizer using the specified file_path and vocab_size.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the input file for training the tokenizer.\n",
    "    - vocab_size (int): The desired vocabulary size for the trained tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    SentencePieceProcessor: A trained BPE tokenizer.\n",
    "    \"\"\"\n",
    "    # Train BPE tokenizer\n",
    "    spm.SentencePieceTrainer.train(input=file_path, model_prefix='bpe_model', model_type='bpe', vocab_size=vocab_size)\n",
    "\n",
    "    # Load the trained BPE tokenizer\n",
    "    model_file = 'bpe_model.model'\n",
    "    bpe_tokenizer = spm.SentencePieceProcessor(model_file)\n",
    "    return bpe_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4532e70d",
   "metadata": {},
   "source": [
    "### NOTE: The list 'tokenizer_list' contains list of tokenizers for Unigram and BPE, this is to be used in Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f8554d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This list contains list of tokenizers for Unigram and BPE, this is to be used in Question 5\"\"\"\n",
    "tokenizer_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8685d6",
   "metadata": {},
   "source": [
    "### Unigram tokenizer for 1k vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b919dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "र: 431118\n",
      "▁के: 328920\n",
      "न: 281629\n",
      "।: 276966\n",
      "ा: 256523\n",
      "ल: 244768\n",
      "▁में: 240292\n",
      "▁: 233755\n",
      "▁है: 215655\n",
      "ी: 211097\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25838\n",
      "('▁है', '▁कि'): 25125\n",
      "('ों', '▁के'): 21724\n",
      "('र', 'ा'): 19993\n",
      "('ों', '▁में'): 19869\n",
      "('▁है', ','): 19551\n",
      "('▁के', '▁साथ'): 18346\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('र', '▁'): 688273\n",
      "('के', '▁'): 372533\n",
      "('▁', 'प'): 350037\n",
      "('▁', 'के'): 328933\n",
      "('▁', 'क'): 323331\n",
      "('न', '▁'): 298802\n",
      "('ने', '▁'): 297998\n",
      "('▁', 'स'): 261329\n",
      "('्', 'र'): 260462\n",
      "('ई', '▁'): 257373\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('अ', '▁'): 2914975\n",
      "('र्', 'अ'): 1710701\n",
      "('अ', '्'): 1382365\n",
      "('ए', '▁'): 1357458\n",
      "('▁', 'क'): 1089979\n",
      "('ई', '▁'): 1027451\n",
      "('आ', '▁'): 984414\n",
      "('क्', 'अ'): 852253\n",
      "('स्', 'अ'): 768895\n",
      "('न्', 'अ'): 705064\n"
     ]
    }
   ],
   "source": [
    "unigram_tokenizer_1k = train_unigram_tokenizer(file_path, 1000)\n",
    "tokenizer_list.append(unigram_tokenizer_1k)  #to be used in Question-5\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies(unigram_tokenizer_1k, data)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "write_to_file_and_print_top_k(model_name=\"unigram\", vocab=\"1000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438c535b",
   "metadata": {},
   "source": [
    "### Unigram tokenizer for 2k vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a34f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "▁के: 324723\n",
      "।: 273666\n",
      "▁में: 240292\n",
      "▁है: 215632\n",
      "▁की: 196738\n",
      ",: 190026\n",
      "ी: 173237\n",
      "▁को: 154736\n",
      "▁से: 151426\n",
      "ल: 146463\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25841\n",
      "('▁है', '▁कि'): 25121\n",
      "('▁है', ','): 19551\n",
      "('ों', '▁के'): 19271\n",
      "('▁के', '▁साथ'): 18346\n",
      "('ों', '▁में'): 17781\n",
      "('▁कहा', '▁कि'): 16097\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('र', '▁'): 688273\n",
      "('के', '▁'): 371528\n",
      "('▁', 'प'): 328908\n",
      "('▁', 'के'): 328435\n",
      "('▁', 'क'): 319830\n",
      "('न', '▁'): 298802\n",
      "('ने', '▁'): 297206\n",
      "('में', '▁'): 253766\n",
      "('▁', 'स'): 253421\n",
      "('▁', 'अ'): 244244\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('अ', '▁'): 2914975\n",
      "('र्', 'अ'): 1586357\n",
      "('अ', '्'): 1382365\n",
      "('ए', '▁'): 1357458\n",
      "('▁', 'क'): 1093480\n",
      "('ई', '▁'): 1027451\n",
      "('आ', '▁'): 984414\n",
      "('क्', 'अ'): 838783\n",
      "('स्', 'अ'): 741573\n",
      "('अं', '▁'): 700936\n"
     ]
    }
   ],
   "source": [
    "unigram_tokenizer_2k = train_unigram_tokenizer(file_path, 2000)\n",
    "tokenizer_list.append(unigram_tokenizer_2k)  #to be used in Question-5\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies(unigram_tokenizer_2k, data)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "write_to_file_and_print_top_k(model_name=\"unigram\", vocab=\"2000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b4146",
   "metadata": {},
   "source": [
    "### BPE tokenizer for 1k vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5251e948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "▁के: 328806\n",
      "।: 270047\n",
      "▁में: 240292\n",
      "▁है: 215519\n",
      "▁की: 199499\n",
      ",: 186960\n",
      "▁को: 162462\n",
      "ल: 158685\n",
      "न: 156906\n",
      "▁से: 152622\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25579\n",
      "('▁है', '▁कि'): 25130\n",
      "('▁है', ','): 19551\n",
      "('▁के', '▁साथ'): 18346\n",
      "('ों', '▁के'): 17977\n",
      "('ने', '▁के'): 16651\n",
      "('▁कहा', '▁कि'): 16100\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('र', '▁'): 688273\n",
      "('के', '▁'): 373261\n",
      "('▁', 'प'): 370443\n",
      "('▁', 'क'): 338393\n",
      "('▁', 'के'): 328820\n",
      "('ने', '▁'): 300414\n",
      "('न', '▁'): 298802\n",
      "('▁', 'स'): 265313\n",
      "('में', '▁'): 252023\n",
      "('्', 'र'): 244352\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('अ', '▁'): 2914975\n",
      "('र्', 'अ'): 1658587\n",
      "('अ', '्'): 1382365\n",
      "('ए', '▁'): 1357458\n",
      "('▁', 'क'): 1074917\n",
      "('ई', '▁'): 1027451\n",
      "('आ', '▁'): 984414\n",
      "('क्', 'अ'): 869382\n",
      "('स्', 'अ'): 758394\n",
      "('अं', '▁'): 700936\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer_1k = train_bpe_tokenizer(file_path, 1000)\n",
    "tokenizer_list.append(bpe_tokenizer_1k)  #to be used in Question-5\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies(bpe_tokenizer_1k, data)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "write_to_file_and_print_top_k(model_name=\"BPE\", vocab=\"1000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f2af7",
   "metadata": {},
   "source": [
    "### BPE tokenizer for 2k vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f3a27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "▁के: 326040\n",
      "।: 270047\n",
      "▁में: 240292\n",
      "▁है: 215519\n",
      "▁की: 197474\n",
      ",: 185205\n",
      "▁को: 156151\n",
      "▁से: 147346\n",
      "▁का: 123398\n",
      ".: 116875\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('▁है', '।'): 89363\n",
      "('▁के', '▁लिए'): 43425\n",
      "('▁हैं', '।'): 33632\n",
      "('▁है', '.'): 25579\n",
      "('▁है', '▁कि'): 25108\n",
      "('▁है', ','): 19551\n",
      "('▁के', '▁साथ'): 18346\n",
      "('▁कहा', '▁कि'): 16096\n",
      "('▁के', '▁बाद'): 14451\n",
      "('ों', '▁के'): 13475\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('र', '▁'): 688273\n",
      "('के', '▁'): 373261\n",
      "('▁', 'प'): 338511\n",
      "('▁', 'के'): 328820\n",
      "('▁', 'क'): 318280\n",
      "('ने', '▁'): 300414\n",
      "('न', '▁'): 298802\n",
      "('▁', 'स'): 257869\n",
      "('में', '▁'): 254647\n",
      "('▁', 'अ'): 244244\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('अ', '▁'): 2914975\n",
      "('र्', 'अ'): 1580578\n",
      "('अ', '्'): 1382365\n",
      "('ए', '▁'): 1357458\n",
      "('▁', 'क'): 1095257\n",
      "('ई', '▁'): 1027451\n",
      "('आ', '▁'): 984414\n",
      "('क्', 'अ'): 821155\n",
      "('स्', 'अ'): 744886\n",
      "('अं', '▁'): 700936\n"
     ]
    }
   ],
   "source": [
    "bpe_tokenizer_2k = train_bpe_tokenizer(file_path, 2000)\n",
    "tokenizer_list.append(bpe_tokenizer_2k) #To be used in Question-5\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies(bpe_tokenizer_2k, data)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "write_to_file_and_print_top_k(model_name=\"BPE\", vocab=\"2000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9de98c",
   "metadata": {},
   "source": [
    "## mBERT and IndicBERT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c0688",
   "metadata": {},
   "source": [
    "###  function definations for counting freqs( only for BERT models) and to get pretrained BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aeeeabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "def count_frequencies_bert(tokens):\n",
    "    \"\"\"\n",
    "    Count the frequencies of unigrams, bigrams, syllable bigrams, and character bigrams in the given list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    - tokens (List[str]): A list of tokens for frequency analysis.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing four Counter objects representing the frequencies of unigrams, bigrams,\n",
    "           syllable bigrams, and character bigrams in the given list of tokens.\n",
    "    \"\"\"\n",
    "    # Count unigram frequencies\n",
    "    unigram_freqs = Counter(tokens)\n",
    "    \n",
    "    bigrams = [(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)]\n",
    "    bigram_freqs = Counter(bigrams)\n",
    "\n",
    "    syllables = []\n",
    "    characters = []\n",
    "    for tok in tokens:\n",
    "        syllables.extend(syllable_convert(tok))\n",
    "        characters.extend(character_conversion(tok))\n",
    "        \n",
    "    bigram_syllables = [(syllables[i], syllables[i + 1]) for i in range(len(syllables) - 1)]\n",
    "    bigram_characters = [(characters[i], characters[i + 1]) for i in range(len(characters) - 1)]\n",
    "    \n",
    "    syllable_freqs = Counter(bigram_syllables)\n",
    "    char_freqs = Counter(bigram_characters)\n",
    "\n",
    "    return unigram_freqs, bigram_freqs, syllable_freqs, char_freqs\n",
    "\n",
    "\n",
    "def trained_mBERT_tokenizer():\n",
    "    \"\"\"\n",
    "    Load and return a pre-trained mBERT (multilingual BERT) tokenizer.\n",
    "\n",
    "    Returns:\n",
    "    BertTokenizer: A pre-trained mBERT tokenizer.\n",
    "    \"\"\"\n",
    "    # Load mBERT tokenizer\n",
    "    return BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f327f66",
   "metadata": {},
   "source": [
    "### mBERT tokenizer for 1k max-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c584f147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "के: 22\n",
      "में: 20\n",
      "।: 18\n",
      "है: 15\n",
      "ब: 15\n",
      "##प: 14\n",
      "आ: 13\n",
      ",: 13\n",
      "को: 13\n",
      "##ी: 12\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('आ', '##प'): 7\n",
      "('ख', '##ोज'): 7\n",
      "('है', '।'): 6\n",
      "('है', 'कि'): 4\n",
      "('हैं', ','): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('##ों', 'की'): 3\n",
      "('##्', '##प'): 3\n",
      "('को', 'ब'): 3\n",
      "('##रल', '##ैंड'): 3\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('#', '#'): 461\n",
      "('#', '्'): 51\n",
      "('#', 'इ'): 40\n",
      "('र', '#'): 31\n",
      "('#', 'ए'): 29\n",
      "('#', 'आ'): 21\n",
      "('#', 'ओ'): 21\n",
      "('स', '#'): 20\n",
      "('क', '#'): 20\n",
      "('आ', '#'): 19\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('#', '#'): 460\n",
      "('अ', '#'): 299\n",
      "('र्', 'अ'): 98\n",
      "('क्', 'अ'): 70\n",
      "('आ', '#'): 52\n",
      "('#', '्'): 51\n",
      "('न्', 'अ'): 50\n",
      "('स्', 'अ'): 49\n",
      "('अ', '्'): 49\n",
      "('प्', 'अ'): 42\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trained_mBERT_tokenizer()\n",
    "\n",
    "# Tokenize the data\n",
    "mBert_encodings = tokenizer(data, max_length=1000, truncation=True)\n",
    "mBert_tokens = tokenizer.convert_ids_to_tokens(mBert_encodings['input_ids'])\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies_bert(mBert_tokens)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    " \n",
    "\"\"\" NOTE: vocab indicates the max-length\"\"\"\n",
    "write_to_file_and_print_top_k(model_name=\"mBERT\", vocab=\"1000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bad1ec",
   "metadata": {},
   "source": [
    "### mBERT tokenizer for 2k max-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8100aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "।: 37\n",
      "में: 36\n",
      "के: 34\n",
      "है: 29\n",
      "ब: 27\n",
      "##र: 26\n",
      "की: 25\n",
      "##ी: 23\n",
      ",: 23\n",
      "ने: 22\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('है', '।'): 12\n",
      "('आ', '##प'): 7\n",
      "('हैं', ','): 7\n",
      "('ख', '##ोज'): 7\n",
      "('हैं', '।'): 6\n",
      "('है', 'कि'): 5\n",
      "('के', 'लिए'): 5\n",
      "('स', '##्व'): 4\n",
      "('##ोज', 'शब्द'): 4\n",
      "('ने', 'कहा'): 4\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('#', '#'): 932\n",
      "('#', '्'): 85\n",
      "('#', 'इ'): 73\n",
      "('र', '#'): 73\n",
      "('#', 'आ'): 52\n",
      "('#', 'ए'): 44\n",
      "('स', '#'): 43\n",
      "('#', 'ओ'): 38\n",
      "('म', '#'): 36\n",
      "('प', '#'): 33\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('#', '#'): 930\n",
      "('अ', '#'): 600\n",
      "('र्', 'अ'): 206\n",
      "('क्', 'अ'): 127\n",
      "('स्', 'अ'): 102\n",
      "('आ', '#'): 101\n",
      "('अ', '्'): 99\n",
      "('न्', 'अ'): 90\n",
      "('#', '्'): 85\n",
      "('प्', 'अ'): 80\n"
     ]
    }
   ],
   "source": [
    "tokenizer = trained_mBERT_tokenizer()\n",
    "# Tokenize the data\n",
    "mBert_encodings = tokenizer(data, max_length=2000, truncation=True)\n",
    "mBert_tokens = tokenizer.convert_ids_to_tokens(mBert_encodings['input_ids'])\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies_bert(mBert_tokens)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "\n",
    "\"\"\" NOTE: vocab indicates the max-length\"\"\"\n",
    "write_to_file_and_print_top_k(model_name=\"mBERT\", vocab=\"2000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201f934",
   "metadata": {},
   "source": [
    "### Loading pretrained IndicBERT tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57ffa722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import collections\n",
    "\n",
    "# Load pretrained IndicBERT Tokenizer\n",
    "def IndicBERT_tokenizer():\n",
    "    indicbert_model_name = \"ai4bharat/indic-bert\"  \n",
    "    indicbert_tokenizer = transformers.AutoTokenizer.from_pretrained(indicbert_model_name)\n",
    "    return indicbert_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4785db",
   "metadata": {},
   "source": [
    "### IndicBERT tokenizer for 1k max-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d347dadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "▁क: 67\n",
      "▁ह: 38\n",
      "▁म: 24\n",
      "।: 23\n",
      "य: 23\n",
      "क: 23\n",
      "▁: 20\n",
      "त: 19\n",
      "न: 16\n",
      "स: 15\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('▁ह', '।'): 11\n",
      "('▁ह', ','): 8\n",
      "('▁क', '▁'): 6\n",
      "('त', '▁ह'): 6\n",
      "('▁ल', 'ए'): 5\n",
      "('न', '▁क'): 5\n",
      "('▁आप', 'क'): 4\n",
      "('▁द', 'य'): 4\n",
      "('य', '▁ह'): 4\n",
      "('▁ह', '▁क'): 4\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('क', '▁'): 99\n",
      "('▁', 'क'): 98\n",
      "('न', '▁'): 60\n",
      "('र', '▁'): 60\n",
      "('▁', 'ह'): 53\n",
      "('ह', '▁'): 50\n",
      "('म', '▁'): 40\n",
      "('▁', 'स'): 38\n",
      "('▁', 'म'): 33\n",
      "('स', '▁'): 32\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('अ', '▁'): 547\n",
      "('क्', 'अ'): 174\n",
      "('र्', 'अ'): 149\n",
      "('अ', 'र्'): 115\n",
      "('▁', 'क्'): 98\n",
      "('न्', 'अ'): 95\n",
      "('स्', 'अ'): 93\n",
      "('ह्', 'अ'): 87\n",
      "('अ', 'न्'): 77\n",
      "('ल्', 'अ'): 77\n"
     ]
    }
   ],
   "source": [
    "indicbert_tokenizer = IndicBERT_tokenizer()\n",
    "\n",
    "ibert_tokens = indicbert_tokenizer(data, max_length=1000, truncation=True)\n",
    "indicbert_tokens = indicbert_tokenizer.convert_ids_to_tokens(ibert_tokens['input_ids'])\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies_bert(indicbert_tokens)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "\n",
    "\"\"\" NOTE: vocab indicates the max-length\"\"\"\n",
    "write_to_file_and_print_top_k(model_name=\"IndicBERT\", vocab=\"1000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e806947e",
   "metadata": {},
   "source": [
    "### IndicBERT tokenizer for 2k max-length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adaf6203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "▁क: 131\n",
      "▁ह: 72\n",
      "य: 52\n",
      "▁म: 48\n",
      "।: 42\n",
      "▁न: 41\n",
      "क: 41\n",
      "▁: 39\n",
      "न: 38\n",
      "त: 35\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('▁ह', '।'): 22\n",
      "('▁ह', ','): 14\n",
      "('▁ल', 'ए'): 10\n",
      "('त', '▁ह'): 10\n",
      "('▁न', 'ह'): 8\n",
      "('▁रह', '▁ह'): 8\n",
      "('▁क', '▁ल'): 8\n",
      "('▁क', '▁'): 7\n",
      "('।', '▁इस'): 7\n",
      "('य', '▁क'): 7\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('▁', 'क'): 190\n",
      "('क', '▁'): 188\n",
      "('र', '▁'): 120\n",
      "('न', '▁'): 117\n",
      "('ह', '▁'): 117\n",
      "('▁', 'ह'): 92\n",
      "('▁', 'स'): 83\n",
      "('म', '▁'): 81\n",
      "('▁', 'म'): 76\n",
      "('त', '▁'): 64\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('अ', '▁'): 1099\n",
      "('क्', 'अ'): 347\n",
      "('र्', 'अ'): 287\n",
      "('अ', 'र्'): 223\n",
      "('न्', 'अ'): 203\n",
      "('स्', 'अ'): 194\n",
      "('▁', 'क्'): 190\n",
      "('ह्', 'अ'): 172\n",
      "('म्', 'अ'): 152\n",
      "('अ', 'न्'): 147\n"
     ]
    }
   ],
   "source": [
    "indicbert_tokenizer = IndicBERT_tokenizer()\n",
    "\n",
    "ibert_tokens = indicbert_tokenizer(data, max_length=2000, truncation=True)\n",
    "indicbert_tokens = indicbert_tokenizer.convert_ids_to_tokens(ibert_tokens['input_ids'])\n",
    "\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies_bert(indicbert_tokens)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "\n",
    "\"\"\" NOTE: vocab indicates the max-length\"\"\"\n",
    "write_to_file_and_print_top_k(model_name=\"IndicBERT\", vocab=\"2000\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed8427",
   "metadata": {},
   "source": [
    "### WhiteSpace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d75c585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 For unigram\n",
      "\n",
      "के: 316314\n",
      "में: 237426\n",
      "की: 189769\n",
      "को: 145266\n",
      "से: 136979\n",
      "और: 114563\n",
      "का: 109743\n",
      "ने: 102378\n",
      "पर: 88560\n",
      "है।: 88260\n",
      "\n",
      "Top 10 For bigram\n",
      "\n",
      "('के', 'लिए'): 42448\n",
      "('है', 'कि'): 24816\n",
      "('के', 'साथ'): 17075\n",
      "('कहा', 'कि'): 15922\n",
      "('के', 'बाद'): 14163\n",
      "('है', 'और'): 10096\n",
      "('ने', 'कहा'): 9098\n",
      "('करने', 'के'): 8826\n",
      "('बताया', 'कि'): 6392\n",
      "('को', 'लेकर'): 5977\n",
      "\n",
      "Top 10 For syllables\n",
      "\n",
      "('र', '्'): 230510\n",
      "('क', 'र'): 176593\n",
      "('स', '्'): 168049\n",
      "('्', 'र'): 167056\n",
      "('क', '्'): 157819\n",
      "('प', '्'): 141260\n",
      "('त', '्'): 123843\n",
      "('औ', 'र'): 115750\n",
      "('प', 'र'): 107644\n",
      "('्', 'य'): 91066\n",
      "\n",
      "Top 10 For char\n",
      "\n",
      "('र्', 'अ'): 1403772\n",
      "('अ', '्'): 1382370\n",
      "('क्', 'अ'): 777423\n",
      "('स्', 'अ'): 686835\n",
      "('अ', 'र्'): 641405\n",
      "('अ', 'क'): 628621\n",
      "('न्', 'अ'): 599585\n",
      "('प्', 'अ'): 547147\n",
      "('त्', 'अ'): 480418\n",
      "('क', 'ए'): 407129\n"
     ]
    }
   ],
   "source": [
    "def whitespace_tokenize(text):\n",
    "    \"\"\"This function implements basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "tokens = whitespace_tokenize(data)\n",
    "# print(tokens)\n",
    "unigram_freqs, bigram_freqs, syllable_freqs, char_freqs = count_frequencies_bert(tokens)\n",
    "freq_arr = [unigram_freqs, bigram_freqs, syllable_freqs, char_freqs]\n",
    "\n",
    "write_to_file_and_print_top_k(model_name=\"WhiteSpace\", vocab=\"\", k=10, freq_arr=freq_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63572dc0",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1db8a2",
   "metadata": {},
   "source": [
    "#### The below cell will require a file named 'cs689_assignment.txt' in current directory for execution, I have added the file in zip submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc68b949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "उन्होंने कहा कि मौजूदा सरकार ने पंजाब को कंगाल बनाकर रख दिया है तथा सरकार विकास कार्यों का बोझ उठाने के योग्य नहीं है जिस कारण राज्य की जनता ने अकाली-भाजपा को सिरे से नकार दिया है।भारत ने इस मामले में वोटिंग ( voting ) न करने का फैसला किया।वजन 162 ग्राममुख्य विपक्षी दल कांग्रेस ने संबित के बयान की आलोचना करते हुए कहा कि यह भाजपा का अहंकार बोल रहा है।मुझे अपने विपक्षी मित्रों के तर्क इस वैज्ञानिक के निष्‍कर्ष के समान ही प्रतीत हो रहे हैं....अगर आपको व्यापार में सफलता मिल रही है तो अपने घर के उत्तर पूर्व कोने को गंगाजल से धोकर वहां हल्दी का स्वास्तिक बनाये। फिर उसकी पूजा करे गुड़ का भोग लगाए। ऐसा करने से आपके व्यापार में सफलता बढेगी साथ ही आप जीवन में सफलता की सीढी भी चढेंगे।नीति आयोग के CEO अमिताभ कांत ने कहा कि देश में पिछले तीन साल में प्रति ग्राहक मोबाइल डाटा कंज्मप्शन में 142 फीसदी की ग्रोथ दर्ज की गई है।भारत अभी टी-20 रैंकिंग में पांचवें स्थान पर है और उसे चौथे स्थान पर पहुंचने के लिये वर्तमान सीरीज 5-0 से जीतनी होगी.लुटने के डर से व्यापारी ने दुकान की जगह थाने से बंटवाई यूरिया इनकी कही गई बातें बेबुनियाद हैं और इनका कोई अर्थ नहीं है. नयी दिल्ली, 27 जुलाई (भाषा) एप्रिल, मे 2018 मध्ये गोपीचंद पवार याने महिलेला फोन करुन पतीबाबत बोलायचे असल्याचे सांगून त्याच्या घरी बोलावले. थीं डिप्रेशन की शिकार उनका यह भी कहना है कि ”1856-57 में भले नमाज़ पढ़ने के सबूत न मिले हों लेकिन 1949 से यहां नमाज़ पढ़ी गई है। इस आतंकी को दिसंबर 1999 में हुए कंधार विमान अपहरण के बाद यात्रियों की सुरक्षित रिहाई के चलते भारत सरकार को मजबूरन छोड़ना पड़ा था। बिहार में 'माननीयों' के आए अच्छे दिन, वेतन-भत्ता बढ़ा हर शुक्रवार को इमरान अपनी गर्लफ्रैंड अवन्तिका और कुछ दोस्‍तों के साथ फिल्‍म देखने चले जाते है। थियेटर में इमरान एक बड़े से टब में पॉनकार्न मंगाते है और फिल्‍म खत्‍म होने तक लगातार खाते रहते है। एक ऐसा करियर (career) चुनना जहाँ आप कुछ रचनात्मक कर सकते है जहाँ आप अपने गुणों को और निखार सकते है वो भी अपने मनपसंद विषयो के साथ। ऐसा … आप अधिकारियों के नाम, उनके पद, शाखा कार्यालय, प्रोफाइल और फोन नंबर की जानकारी प्राप्त कर सकते हैं। चाणक्य निति- जो व्यक्ति शास्त्रों के सूत्रों का अभ्यास करके ज्ञान ग्रहण करेगा उसे अत्यंत वैभवशाली कर्तव्य के सिद्धांत ज्ञात होगे। उसे इस बात का पता चलेगा कि किन बातों का अनुशरण करना चाहिए और किनका नहीं। आध्यात्म-मार्ग पर हम जिन जटिलताओं सामना करते हैं ये मार्ग के कारण नहीं होतीं। हाथ खाली हैं. ‘विश्‍वशांती, अखंडता, महाराष्ट्रावरील दुष्काळाचे संकट हटू दे’,  अशी प्रार्थना करण्यात आली. संघ के प्रांतीय सचिव एवं कोषाध्यक्ष रणदीप आर्य ने बताया कि बिजली विभाग में कभी ट्रांसफार्मर दिलवाने के नाम पर और कभी तेल व केबल के नाम पर अतिरिक्त शुल्क मांगा जाता है और किसान को मजबूरीवश यह शुल्क देना पड़ता है। शपथ ग्रहण से पहले बीजेपी में बड़े बदलाव हो सकते हैं।\n"
     ]
    }
   ],
   "source": [
    "sentences = \"\"\n",
    "sentences_file = 'cs689_assignment.txt'\n",
    "\n",
    "with open(sentences_file, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        if line !=\"\\n\":\n",
    "            temp = line.strip()\n",
    "            sentences = sentences+temp[3:]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23537e76",
   "metadata": {},
   "source": [
    "### Functions for Precision, Recall and F1-Score  \n",
    "(In these definations, doc string is not added because the functions are self-explanatory with relevant comment added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f730c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(true_tokens, pred_tokens):\n",
    "    true_positives = len(set(true_tokens) & set(pred_tokens))\n",
    "    false_positives = len(pred_tokens) - true_positives\n",
    "    \n",
    "    #case where precision is being dividing by zero\n",
    "    if true_positives + false_positives == 0:\n",
    "        return 0 \n",
    "    \n",
    "    precision_value = true_positives / (true_positives + false_positives)\n",
    "    return precision_value\n",
    "\n",
    "def recall(true_tokens, pred_tokens):\n",
    "    true_positives = len(set(true_tokens) & set(pred_tokens))\n",
    "    false_negatives = len(true_tokens) - true_positives\n",
    "    \n",
    "    #case where recall is being dividing by zero\n",
    "    if true_positives + false_negatives == 0:\n",
    "        return 0  \n",
    "    \n",
    "    recall_value = true_positives / (true_positives + false_negatives)\n",
    "    return recall_value\n",
    "\n",
    "def f1_score(true_tokens, pred_tokens):\n",
    "    precision_value = precision(true_tokens, pred_tokens)\n",
    "    recall_value = recall(true_tokens, pred_tokens)\n",
    "    \n",
    "    # Handle the case where F1-Score dividing by zero\n",
    "    if precision_value + recall_value == 0:\n",
    "        return 0  \n",
    "    \n",
    "    f1_score_value = 2 * (precision_value * recall_value) / (precision_value + recall_value)\n",
    "    return f1_score_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bb7652",
   "metadata": {},
   "source": [
    "#### The below cell will require a file named 'ground_truth_tokens.txt'  in current directory for execution, I have added the file in zip submission\n",
    "NOTE: The file downloaded from website having question-3 will need cleaning as that contains questions too but here only word-groups representing truth tokens will be needed, that's why cleaned file is attached with zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6e5403cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of True Tokens: ['उन्होंने', 'कहा कि', 'मौजूदा', 'सरकार ने', 'पंजाब को', 'कंगाल', 'बनाकर', 'रख', 'दिया है', 'तथा', 'सरकार', 'विकास', 'कार्यों का', 'बोझ', 'उठाने के', 'योग्य', 'नहीं', 'है', 'जिस', 'कारण', 'राज्य की', 'जनता ने', 'अकाली-भाजपा को', 'सिरे से', 'नकार', 'दिया है।\\n\\n\\nभारत ने', 'इस', 'मामले में', 'वोटिंग', 'न', 'करने का', 'फैसला', 'किया।\\n\\n\\nवजन', '162 ग्राम\\n\\n\\nमुख्य', 'विपक्षी दल', 'कांग्रेस ने', 'संबित के', 'बयान की', 'आलोचना', 'करते हुए', 'कहा कि', 'यह', 'भाजपा का', 'अहंकार', 'बोल', 'रहा है।', 'मुझे', 'अपने', 'विपक्षी', 'मित्रों के']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'ground_truth_tokens.txt'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    tokens = file.read().split(',')\n",
    "\n",
    "true_tokens = [word.strip() for word in tokens if word!=\" \"]\n",
    "print(\"List of True Tokens:\", true_tokens[:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c8576",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for Unigram(1K,2K) and BPE (1K,2K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "17a12e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for Unigram_1k: 0.06\n",
      "Recall for Unigram_1k: 0.19\n",
      "F1-Score for Unigram_1k: 0.09\n",
      "\n",
      "\n",
      "Precision for Unigram_2k: 0.09\n",
      "Recall for Unigram_2k: 0.25\n",
      "F1-Score for Unigram_2k: 0.14\n",
      "\n",
      "\n",
      "Precision for BPE_1k: 0.05\n",
      "Recall for BPE_1k: 0.16\n",
      "F1-Score for BPE_1k: 0.08\n",
      "\n",
      "\n",
      "Precision for BPE_2k: 0.08\n",
      "Recall for BPE_2k: 0.23\n",
      "F1-Score for BPE_2k: 0.12\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"  data_file is path to file of 25 sentences\n",
    "     unigram_tokenizer is tokenizer for unigram model   \"\"\"\n",
    "\n",
    "\"\"\" Tokenizer list contains the trained tokenizer (in Question 4) for Unigram and BPE\"\"\"\n",
    "\n",
    "model_name = [\"Unigram_1k\",\"Unigram_2k\",\"BPE_1k\",\"BPE_2k\"]\n",
    "index = 0\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    pred_tokens = tokenizer.encode(sentences, out_type=str)\n",
    "    #  Removing '_' from staring of tokens,'_' works just as seperator so it will not affect the quality of tokens\n",
    "    pred_tokens = [token[1:] if token.startswith('▁') else token for token in pred_tokens]\n",
    "    \n",
    "    # Precision\n",
    "    precision_value = precision(true_tokens, pred_tokens)\n",
    "    print(f\"Precision for {model_name[index]}: {precision_value:.2f}\")\n",
    "\n",
    "    # Recall\n",
    "    recall_value = recall(true_tokens, pred_tokens)\n",
    "    print(f\"Recall for {model_name[index]}: {recall_value:.2f}\")\n",
    "\n",
    "    # F1-score\n",
    "    f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "    print(f\"F1-Score for {model_name[index]}: {f1_score_value:.2f}\")\n",
    "    \n",
    "    index= index+1\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bb2e46",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for mBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8321fd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.06\n",
      "Recall: 0.18\n",
      "F1-Score: 0.09\n"
     ]
    }
   ],
   "source": [
    "# NO need of tokenizer list, because pretrained tok is used everywhere\n",
    "mBERT_tokenizer = trained_mBERT_tokenizer()\n",
    "mBert_encodings = mBERT_tokenizer.encode_plus(sentences, max_length=1000, truncation=True, padding=True)\n",
    "pred_tokens = mBERT_tokenizer.convert_ids_to_tokens(mBert_encodings['input_ids'])\n",
    "\n",
    "# Precision\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9833d3f8",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for IndicBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c4bce89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.02\n",
      "Recall: 0.04\n",
      "F1-Score: 0.02\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NO need of tokenizer list, because pretrained tok is used everywhere\n",
    "indicbert_tokenizer = IndicBERT_tokenizer()\n",
    "ibert_tokens = indicbert_tokenizer.encode_plus(sentences, max_length=1000, truncation=True)\n",
    "pred_tokens = indicbert_tokenizer.convert_ids_to_tokens(ibert_tokens['input_ids'])\n",
    "pred_tokens = [token[1:] if token.startswith('▁') else token for token in pred_tokens]\n",
    "\n",
    "# Precision\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd484af",
   "metadata": {},
   "source": [
    "### Precision, Recall and F1-Score for WhiteSpace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e2178ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.36\n",
      "Recall: 0.52\n",
      "F1-Score: 0.42\n"
     ]
    }
   ],
   "source": [
    "pred_tokens = whitespace_tokenize(sentences)\n",
    "# print(pred_tokens)\n",
    "\n",
    "# Precision\n",
    "precision_value = precision(true_tokens, pred_tokens)\n",
    "print(f\"Precision: {precision_value:.2f}\")\n",
    "\n",
    "# Recall\n",
    "recall_value = recall(true_tokens, pred_tokens)\n",
    "print(f\"Recall: {recall_value:.2f}\")\n",
    "\n",
    "# F1-score\n",
    "f1_score_value = f1_score(true_tokens, pred_tokens)\n",
    "print(f\"F1-Score: {f1_score_value:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbf02e",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb265b93",
   "metadata": {},
   "source": [
    "# Problem-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33479b",
   "metadata": {},
   "source": [
    "### Observations from comparison:\n",
    "    On assessing different tokenizers to tokenizing Hindi sentences, the whitespace tokenizer clearly excels with its high precision, demonstrating its ability to capture the structures of the hindi language which make sense because in hindi words generally makes more sense when tokenized by space as delimeter . Unigram and BPE tokenizers also exhibit moderate performance, while mBERT and IndicBERT tokenizers perform more or less similarly. The prominence of whitespace tokenization highlights that, despite advancements in tokenization methods, the use of whitespace remains effective in accurately representing the Hindi language processing. This suggests that while tokenizers are valuable tools, there is still room for growth and whitespace tokenization still remains a reliable option for working with Hindi text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
